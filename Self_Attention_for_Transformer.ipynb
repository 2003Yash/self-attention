{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHgrvtcgBiZ40rgnwlgTqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/self-attention/blob/main/Self_Attention_for_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "self attention: we use q,k,v vectors to create attention matrix (input_len x input_len) and we multiply them and use softmax to get attention matrix for word context inportance with other words in how much each word should look into other words in the same sentence"
      ],
      "metadata": {
        "id": "LWmniHOiUFYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXIxFGD9T-jZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "L, d_k, d_v = 4, 8, 8 # l = length of string and size if each vector = 8(no specific reason for 8)\n",
        "q = np.random.randn(L, d_k)\n",
        "k = np.random.randn(L, d_k)\n",
        "v = np.random.randn(L, d_v)\n",
        "#all 8x1 vectors randomly assigned values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUIMPu87UQEc",
        "outputId": "318a6a2b-372c-4e59-c243-2d4874cdadca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-1.79708796  0.52122516  0.34239011 -0.2192233   0.95937318  0.01950385\n",
            "  -0.41948086  0.19684975]\n",
            " [-0.28763054  1.56560955 -1.50161045  0.3472905  -0.31556079  1.35173172\n",
            "  -1.14744159  0.26172031]\n",
            " [ 0.49501539  0.83643826 -0.61257021 -0.46052311  0.21198769 -1.21393321\n",
            "   0.68011574  1.08998828]\n",
            " [ 1.04793736  0.90690933  1.12068841  0.96005338 -0.48338012 -0.49387283\n",
            "   0.89347304 -0.98213068]]\n",
            "K\n",
            " [[-1.69457653  1.14189547 -0.54117259 -0.21483296 -1.06478926 -2.33830904\n",
            "  -0.53044194 -2.02805418]\n",
            " [-0.98502234  0.65551073  0.3332141  -0.5899652  -0.47458067 -1.74058004\n",
            "  -1.61271634  0.83231212]\n",
            " [ 0.5630061  -0.12879049 -0.89508148  1.07619482  2.16600192  0.2716528\n",
            "  -0.04767808  2.50152317]\n",
            " [ 0.13802501  0.90824179 -0.63588164 -2.5203722   0.31566198  0.4199465\n",
            "  -1.52188367 -1.18428275]]\n",
            "V\n",
            " [[-0.2684259   0.74669319  0.25532415  0.54156151  0.12737344  0.85743961\n",
            "  -0.56087513 -0.44426863]\n",
            " [-0.04093924  1.80102587 -0.27960843 -1.57039158  2.52074117 -0.07819322\n",
            "   0.89218422  0.3790867 ]\n",
            " [ 0.03062668 -1.05183871  0.2217064   0.5625631  -1.65274518  1.60354294\n",
            "  -0.80843236  0.65860169]\n",
            " [ 0.94560063 -0.66219313 -0.01830485  0.59928149  0.2519207  -1.08885896\n",
            "  -1.64722027 -0.33519226]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(q, k.T)\n",
        "#4x4 aresult since the sequence is 4x4 and it describes attendtion of each other word in sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT_mC07zUTuJ",
        "outputId": "5882354c-49d6-4298-ec61-5d738ac42163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.25844396,  2.7063599 ,  0.97443233,  1.27646365],\n",
              "       [ 0.26630277,  0.46964187,  1.74734626,  3.36616325],\n",
              "       [ 0.58823693,  1.95098796,  3.04726077, -0.39055741],\n",
              "       [ 1.63445254, -1.80005162, -3.17729507, -2.72060903]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var(), k.var(), np.matmul(q, k.T).var()\n",
        "#since varience is too large for thrid value we divide it with sq_rt(dimension size) to bring all variances in a reasonable range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97c5QY6XUYbZ",
        "outputId": "a697749e-c380-4f64-9a47-9c33884d1a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7373171315861786, 1.4392136315212765, 3.586251586314168)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "q.var(), k.var(), scaled.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3TMA8UFUaXv",
        "outputId": "fe2368e8-5fb1-4b1e-ee1c-0b99faa1908a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7373171315861786, 1.4392136315212765, 0.44828144828927097)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled # normalized vector of attention of other words in the line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdn0HyaUUay1",
        "outputId": "a49108f7-8564-4a42-a76f-abe61e27e311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.79848052,  0.95684272,  0.34451385,  0.45129805],\n",
              "       [ 0.09415225,  0.16604347,  0.61778019,  1.19011843],\n",
              "       [ 0.20797316,  0.68977841,  1.07736938, -0.1380829 ],\n",
              "       [ 0.57786624, -0.63641435, -1.12334345, -0.96188055]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need mask so decoder won't focus on next since we don't even have the next we still nave to predict it\n",
        "mask = np.tril(np.ones( (L, L) ))\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqxNO3wjUdJH",
        "outputId": "e3c308eb-31aa-4be1-c79d-350008691f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask[mask == 0] = -np.infty\n",
        "mask[mask == 1] = 0\n",
        "# since after adding mask to scaled self attention vector 0 will do no change and -int will always be -int"
      ],
      "metadata": {
        "id": "-yMyJKyEUf3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_gcKLCfUihS",
        "outputId": "9c952f27-ad2e-45ff-8613-3acf3ba88de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled + mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C4cArPOUknY",
        "outputId": "96d3d16b-e112-4ecf-8de1-d177d90dbd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.79848052,        -inf,        -inf,        -inf],\n",
              "       [ 0.09415225,  0.16604347,        -inf,        -inf],\n",
              "       [ 0.20797316,  0.68977841,  1.07736938,        -inf],\n",
              "       [ 0.57786624, -0.63641435, -1.12334345, -0.96188055]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to convert vector to probability distribution horizontally so all rows add upto 1 and each prob discribes that word importance iin attention\n",
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
      ],
      "metadata": {
        "id": "IDKBjMSXUq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = softmax(scaled + mask)"
      ],
      "metadata": {
        "id": "M87Is49UUqG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naW4WXHDU2SN",
        "outputId": "2573f1cc-5ddc-4953-f167-4795c77e708f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.48203493, 0.51796507, 0.        , 0.        ],\n",
              "       [0.19982158, 0.32351002, 0.4766684 , 0.        ],\n",
              "       [0.59038094, 0.175298  , 0.10772248, 0.12659857]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiply value with q.k to get perfect context filled attention vector\n",
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AkeScPyU4UR",
        "outputId": "81a83c15-6419-4f7f-b249-a9c253b37e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.2684259 ,  0.74669319,  0.25532415,  0.54156151,  0.12737344,\n",
              "         0.85743961, -0.56087513, -0.44426863],\n",
              "       [-0.15059575,  1.29280069, -0.02175224, -0.55235642,  1.36705433,\n",
              "         0.37281449,  0.19175886, -0.01779933],\n",
              "       [-0.05228277,  0.23047706,  0.06624358, -0.13166569,  0.0531256 ,\n",
              "         0.91039689, -0.20879857,  0.3477985 ],\n",
              "       [-0.04263923,  0.55941029,  0.12328911,  0.18090996,  0.37093474,\n",
              "         0.52739853, -0.47035395, -0.16732325]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
        "\n",
        "#same above code but for encoder without the mask\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "  d_k = q.shape[-1]\n",
        "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scaled = scaled + mask\n",
        "  attention = softmax(scaled)\n",
        "  out = np.matmul(attention, v)\n",
        "  return out, attention"
      ],
      "metadata": {
        "id": "OGUfOz1fWEeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"New V\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhSLyHZqU7yU",
        "outputId": "8eee68e4-8277-499a-ec97-0b68127c0be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q\n",
            " [[-1.79708796  0.52122516  0.34239011 -0.2192233   0.95937318  0.01950385\n",
            "  -0.41948086  0.19684975]\n",
            " [-0.28763054  1.56560955 -1.50161045  0.3472905  -0.31556079  1.35173172\n",
            "  -1.14744159  0.26172031]\n",
            " [ 0.49501539  0.83643826 -0.61257021 -0.46052311  0.21198769 -1.21393321\n",
            "   0.68011574  1.08998828]\n",
            " [ 1.04793736  0.90690933  1.12068841  0.96005338 -0.48338012 -0.49387283\n",
            "   0.89347304 -0.98213068]]\n",
            "K\n",
            " [[-1.69457653  1.14189547 -0.54117259 -0.21483296 -1.06478926 -2.33830904\n",
            "  -0.53044194 -2.02805418]\n",
            " [-0.98502234  0.65551073  0.3332141  -0.5899652  -0.47458067 -1.74058004\n",
            "  -1.61271634  0.83231212]\n",
            " [ 0.5630061  -0.12879049 -0.89508148  1.07619482  2.16600192  0.2716528\n",
            "  -0.04767808  2.50152317]\n",
            " [ 0.13802501  0.90824179 -0.63588164 -2.5203722   0.31566198  0.4199465\n",
            "  -1.52188367 -1.18428275]]\n",
            "V\n",
            " [[-0.2684259   0.74669319  0.25532415  0.54156151  0.12737344  0.85743961\n",
            "  -0.56087513 -0.44426863]\n",
            " [-0.04093924  1.80102587 -0.27960843 -1.57039158  2.52074117 -0.07819322\n",
            "   0.89218422  0.3790867 ]\n",
            " [ 0.03062668 -1.05183871  0.2217064   0.5625631  -1.65274518  1.60354294\n",
            "  -0.80843236  0.65860169]\n",
            " [ 0.94560063 -0.66219313 -0.01830485  0.59928149  0.2519207  -1.08885896\n",
            "  -1.64722027 -0.33519226]]\n",
            "New V\n",
            " [[-0.2684259   0.74669319  0.25532415  0.54156151  0.12737344  0.85743961\n",
            "  -0.56087513 -0.44426863]\n",
            " [-0.15059575  1.29280069 -0.02175224 -0.55235642  1.36705433  0.37281449\n",
            "   0.19175886 -0.01779933]\n",
            " [-0.05228277  0.23047706  0.06624358 -0.13166569  0.0531256   0.91039689\n",
            "  -0.20879857  0.3477985 ]\n",
            " [-0.04263923  0.55941029  0.12328911  0.18090996  0.37093474  0.52739853\n",
            "  -0.47035395 -0.16732325]]\n",
            "Attention\n",
            " [[1.         0.         0.         0.        ]\n",
            " [0.48203493 0.51796507 0.         0.        ]\n",
            " [0.19982158 0.32351002 0.4766684  0.        ]\n",
            " [0.59038094 0.175298   0.10772248 0.12659857]]\n"
          ]
        }
      ]
    }
  ]
}